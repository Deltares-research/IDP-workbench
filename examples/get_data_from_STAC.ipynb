{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Notebook to load multiple dataset from different STAC catalogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created by EtiÃ«nne Kras, 24-10-2024, using geo_env\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import pystac_client\n",
    "import xarray as xr\n",
    "import rioxarray as rio\n",
    "import numpy as np\n",
    "\n",
    "from copy import deepcopy\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function(s)\n",
    "\n",
    "# function to put items in dataframe\n",
    "def items_to_dataframe(items: List[Dict]) -> pd.DataFrame:\n",
    "    \"\"\"STAC items to Pandas dataframe.\n",
    "\n",
    "    Args:\n",
    "        items (List[Dict]): _description_\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: _description_\n",
    "    \"\"\"\n",
    "    _items = []\n",
    "    for i in items:\n",
    "        _i = deepcopy(i)\n",
    "        # _i['geometry'] = shape(_i['geometry'])\n",
    "        # ...  # for example, drop some attributes that you're not interested in\n",
    "        _items.append(_i)\n",
    "    df = pd.DataFrame(pd.json_normalize(_items))\n",
    "    # for field in [\"properties.datetime\"]:\n",
    "    #     if field in df:\n",
    "    #         df[field] = pd.to_datetime(df[field])\n",
    "    # df = df.sort_values(\"properties.datetime\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open data from first catalog (future shorelines & SLR projections AR6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the CoCliCo STAC catalog\n",
    "catalog = pystac_client.Client.open(\n",
    "    \"https://storage.googleapis.com/coclico-data-public/coclico/coclico-stac/catalog.json\"\n",
    ")\n",
    "# catalog\n",
    "# see visually here: radiantearth.github.io/stac-browser/#/external/storage.googleapis.com/coclico-data-public/coclico/coclico-stac-4oct/catalog.json\n",
    "\n",
    "# list the datasets present in the catalog, we are interested in the slp5 and slp6 sets\n",
    "list(catalog.get_children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar6_col = catalog.get_child(\"slp6\")\n",
    "parquet_asset = ar6_col.get_assets()[\"geoparquet-stac-items\"]\n",
    "\n",
    "# Load the parquet file into memory\n",
    "ar6_parquet_df = pd.read_parquet(parquet_asset.href)\n",
    "ar6_parquet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# read STAC items as Pandas dataframe\n",
    "\n",
    "# TODO: how to speed up?? \n",
    "# select one ens ..\n",
    "\n",
    "# AR6, takes a while +/- 6 min\n",
    "ar6_col = catalog.get_child(\"slp6\")\n",
    "items_ar6 = list(ar6_col.get_items()) # this is slow as we need to list all items\n",
    "items_ar6_df = items_to_dataframe([i.to_dict() for i in items_ar6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yr = 2100  # set year\n",
    "ens = 50  # set ensemble [0-100]\n",
    "var = \"slr\"  # set variable\n",
    "ccs6 = \"1-26\"  # set climate change scenario for AR6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter items in datasets\n",
    "\n",
    "# define variables\n",
    "ens_list = [\"5\", \"50\", \"95\"]  # ensemble list to look into\n",
    "yrs_list = np.arange(1970, 2200, 10)  # years to look into (step of 10 years from 1970)\n",
    "key_list = [\"CCS\", \"YRS\", \"ENS\"]\n",
    "\n",
    "# index AR6 dataframe on criteria\n",
    "fil_idx6 = []\n",
    "AR6_dict = {key: [] for key in key_list}\n",
    "for idx, i in enumerate(items_ar6_df.id):\n",
    "    enss = str(i).split(\"/\")[1].split(\"ens\")[-1]  # ensemble\n",
    "    yrs = int(str(i).split(\"/\")[2][0:4])  # yrs\n",
    "    ccs = str(i).split(\"/\")[0].split(\"=\")[-1] # ccs\n",
    "    if enss in [str(float(x)) for x in ens_list] and yrs in yrs_list:  # constraining read ensembles and years\n",
    "        AR6_dict[\"CCS\"].append(ccs)\n",
    "        AR6_dict[\"YRS\"].append(yrs)\n",
    "        AR6_dict[\"ENS\"].append(enss)\n",
    "        fil_idx6.append(idx)\n",
    "\n",
    "# filter AR6 dataframe and STAC items on index\n",
    "items_df_fil6 = items_ar6_df.filter(items = fil_idx6, axis=0)\n",
    "items_fil6 = [items_ar6[i] for i in fil_idx6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "\n",
    "# get AR6 collection and item href\n",
    "for i in items_fil6:\n",
    "    if i.id == r\"ssp=%s/%s_ens%s/%s.tif\" % (ccs6, var, float(ens), yr):\n",
    "        ar6_item_href = i.assets[\"data\"].href\n",
    "\n",
    "ar6_item = rio.open_rasterio(ar6_item_href, masked=True)\n",
    "ar6_item_corr = ar6_item / 1000\n",
    "ar6_item_corr.plot()\n",
    "\n",
    "# cbar limits\n",
    "# vmin = max(\n",
    "#     min(np.nanmin(ar5_item), np.nanmin(ar6_item_corr)), -0.2\n",
    "# )  # bound to -0.2 if smaller than this value\n",
    "# vmax = max(np.nanmax(ar5_item), np.nanmax(ar6_item_corr))\n",
    "\n",
    "# colormap\n",
    "#cwd = pathlib.Path().resolve().parent\n",
    "#slev_divl = np.loadtxt(str(pathlib.Path.joinpath(cwd, r\"src/coclico/colormaps/slev_div.txt\")))\n",
    "#slev_div = mcolors.LinearSegmentedColormap.from_list('slev_div', slev_divl/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_col = catalog.get_child(\"sc\")\n",
    "href_sc = sc_col.assets[\"data\"].href\n",
    "\n",
    "# Printing the dataset object shows the storm surge level consists of three dimensions.\n",
    "# Every storm surge level is associated with a certain station, scenario and revisting period.\n",
    "ds = xr.open_zarr(href_sc)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot on map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open data from second catalog (world pop & subsidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the GCA SOTC STAC catalog\n",
    "catalog2 = pystac_client.Client.open(\n",
    "    \"https://storage.googleapis.com/dgds-data-public/gca/SOTC/gca-sotc/catalog.json\"\n",
    ")\n",
    "# catalog\n",
    "# see visually here: https://radiantearth.github.io/stac-browser/#/external/storage.googleapis.com/dgds-data-public/gca/SOTC/gca-sotc/catalog.json\n",
    "\n",
    "# list the datasets present in the catalog, we are interested in the slp5 and slp6 sets\n",
    "list(catalog2.get_children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# read STAC items as Pandas dataframe\n",
    "\n",
    "# TODO: how to speed up?? \n",
    "# select one ens ..\n",
    "\n",
    "# AR6, takes a while +/- 6 min\n",
    "sub_col = catalog2.get_child(\"Haz-Land_Sub_2040_COGs\")\n",
    "items_sub = list(sub_col.get_items()) # this is slow as we need to list all items\n",
    "items_sub_df = items_to_dataframe([i.to_dict() for i in items_sub])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot on map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parquet file\n",
    "\n",
    "pop_col = catalog2.get_child(\"Exp_world_pop_parquet\")\n",
    "\n",
    "# TODO: open parquet file\n",
    "href_pop = pop_col.assets[\"data\"].href"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet('example_pa.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot on map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idp-dashboard",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
